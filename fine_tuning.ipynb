{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10860348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3100, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3155, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3367, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3612, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3672, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_7138/1486010956.py\", line 3, in <module>\n",
      "    import evaluate\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/evaluate/__init__.py\", line 29, in <module>\n",
      "    from .evaluation_suite import EvaluationSuite\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/evaluate/evaluation_suite/__init__.py\", line 10, in <module>\n",
      "    from ..evaluator import evaluator\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/evaluate/evaluator/__init__.py\", line 17, in <module>\n",
      "    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 33, in <module>\n",
      "    from .generic import (\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/transformers/utils/generic.py\", line 465, in <module>\n",
      "    import torch.utils._pytree as _torch_pytree\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/lukeg/ClimatEnv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/lukeg/ClimatEnv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from preprocessing import climate_fever_to_claim_evidence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e8a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# This should always output true now, but worth checking\n",
    "print(f\"is GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9720bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['claim_id', 'claim', 'claim_label', 'evidences'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The climate-fever dataset is in a format which the transformers Trainer does not understand\n",
    "# It must be preprocessed using the functions in preprocessing.py\n",
    "\n",
    "df = pd.read_json(\"data/climate_fever/climate-fever-dataset-sample.jsonl\", lines=True)\n",
    "preprocessed_df = climate_fever_to_claim_evidence_pairs(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2264f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF\n",
      "    claim_id                                              claim claim_label  \\\n",
      "0         0  Global warming is driving polar bears toward e...    SUPPORTS   \n",
      "1         5  The sun has gone into ‘lockdown’ which could c...    SUPPORTS   \n",
      "2         6        The polar bear population has been growing.     REFUTES   \n",
      "3         9  Ironic' study finds more CO2 has slightly cool...     REFUTES   \n",
      "4        10  Human additions of CO2 are in the margin of er...     REFUTES   \n",
      "\n",
      "                                           evidences  \n",
      "0  [{'evidence_id': 'Extinction risk from global ...  \n",
      "1  [{'evidence_id': 'Famine:386', 'evidence_label...  \n",
      "2  [{'evidence_id': 'Polar bear:1332', 'evidence_...  \n",
      "3  [{'evidence_id': 'Atmosphere of Mars:131', 'ev...  \n",
      "4  [{'evidence_id': 'Carbon dioxide in Earth's at...  \n",
      "\n",
      "EXPLODED\n",
      "    claim_id                                              claim claim_label  \\\n",
      "0         0  Global warming is driving polar bears toward e...    SUPPORTS   \n",
      "1         0  Global warming is driving polar bears toward e...    SUPPORTS   \n",
      "2         0  Global warming is driving polar bears toward e...    SUPPORTS   \n",
      "3         0  Global warming is driving polar bears toward e...    SUPPORTS   \n",
      "4         0  Global warming is driving polar bears toward e...    SUPPORTS   \n",
      "\n",
      "                                           evidences  \n",
      "0  {'evidence_id': 'Extinction risk from global w...  \n",
      "1  {'evidence_id': 'Global warming:14', 'evidence...  \n",
      "2  {'evidence_id': 'Global warming:178', 'evidenc...  \n",
      "3  {'evidence_id': 'Habitat destruction:61', 'evi...  \n",
      "4  {'evidence_id': 'Polar bear:1328', 'evidence_l...  \n",
      "\n",
      "Columns: Index(['claim_id', 'claim', 'claim_label', 'evidences'], dtype='object')\n",
      "\n",
      "PREPROCESSED\n",
      "    claim_id                                              claim  \\\n",
      "0         0  Global warming is driving polar bears toward e...   \n",
      "1         0  Global warming is driving polar bears toward e...   \n",
      "2         0  Global warming is driving polar bears toward e...   \n",
      "3         0  Global warming is driving polar bears toward e...   \n",
      "4         0  Global warming is driving polar bears toward e...   \n",
      "\n",
      "                               evidence_id   evidence_label  \\\n",
      "0  Extinction risk from global warming:170  NOT_ENOUGH_INFO   \n",
      "1                        Global warming:14         SUPPORTS   \n",
      "2                       Global warming:178  NOT_ENOUGH_INFO   \n",
      "3                   Habitat destruction:61         SUPPORTS   \n",
      "4                          Polar bear:1328  NOT_ENOUGH_INFO   \n",
      "\n",
      "                                            evidence   entropy  \n",
      "0  \"Recent Research Shows Human Activity Driving ...  0.693147  \n",
      "1  Environmental impacts include the extinction o...  0.000000  \n",
      "2  Rising temperatures push bees to their physiol...  0.693147  \n",
      "3  Rising global temperatures, caused by the gree...  0.000000  \n",
      "4    \"Bear hunting caught in global warming debate\".  0.693147  \n",
      "\n",
      "Columns: Index(['claim_id', 'claim', 'evidence_id', 'evidence_label', 'evidence',\n",
      "       'entropy'],\n",
      "      dtype='object')\n",
      "\n",
      "claim_id                                                          0\n",
      "claim             Global warming is driving polar bears toward e...\n",
      "evidence_id                                       Global warming:14\n",
      "evidence_label                                             SUPPORTS\n",
      "evidence          Environmental impacts include the extinction o...\n",
      "entropy                                                         0.0\n",
      "Name: 1, dtype: object\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m df = pd.read_json(\u001b[33m\"\u001b[39m\u001b[33mdata/climate_fever/climate-fever-dataset-sample.jsonl\u001b[39m\u001b[33m\"\u001b[39m, lines=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m preprocessed_df = climate_fever_to_claim_evidence_pairs(df)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m dataset = \u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/datasets/arrow_dataset.py:841\u001b[39m, in \u001b[36mDataset.from_pandas\u001b[39m\u001b[34m(cls, df, features, info, split, preserve_index)\u001b[39m\n\u001b[32m    839\u001b[39m     info = DatasetInfo()\n\u001b[32m    840\u001b[39m info.features = features\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m table = \u001b[43mInMemoryTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    846\u001b[39m     \u001b[38;5;66;03m# more expensive cast than InMemoryTable.from_pandas(..., schema=features.arrow_schema)\u001b[39;00m\n\u001b[32m    847\u001b[39m     \u001b[38;5;66;03m# needed to support the str to Audio conversion for instance\u001b[39;00m\n\u001b[32m    848\u001b[39m     table = table.cast(features.arrow_schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/datasets/table.py:720\u001b[39m, in \u001b[36mInMemoryTable.from_pandas\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pandas\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args, **kwargs):\n\u001b[32m    666\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    667\u001b[39m \u001b[33;03m    Convert pandas.DataFrame to an Arrow Table.\u001b[39;00m\n\u001b[32m    668\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    718\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    719\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/pyarrow/table.pxi:4793\u001b[39m, in \u001b[36mpyarrow.lib.Table.from_pandas\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/pyarrow/pandas_compat.py:595\u001b[39m, in \u001b[36mdataframe_to_arrays\u001b[39m\u001b[34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[39m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdataframe_to_arrays\u001b[39m(df, schema, preserve_index, nthreads=\u001b[32m1\u001b[39m, columns=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    587\u001b[39m                         safe=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    588\u001b[39m     (all_names,\n\u001b[32m    589\u001b[39m      column_names,\n\u001b[32m    590\u001b[39m      column_field_names,\n\u001b[32m    591\u001b[39m      index_column_names,\n\u001b[32m    592\u001b[39m      index_descriptors,\n\u001b[32m    593\u001b[39m      index_columns,\n\u001b[32m    594\u001b[39m      columns_to_convert,\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m      convert_fields) = \u001b[43m_get_columns_to_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m                                               \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m     \u001b[38;5;66;03m# NOTE(wesm): If nthreads=None, then we use a heuristic to decide whether\u001b[39;00m\n\u001b[32m    599\u001b[39m     \u001b[38;5;66;03m# using a thread pool is worth it. Currently the heuristic is whether the\u001b[39;00m\n\u001b[32m    600\u001b[39m     \u001b[38;5;66;03m# nrows > 100 * ncols and ncols > 1.\u001b[39;00m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nthreads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/pyarrow/pandas_compat.py:372\u001b[39m, in \u001b[36m_get_columns_to_convert\u001b[39m\u001b[34m(df, schema, preserve_index, columns)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_columns_to_convert\u001b[39m(df, schema, preserve_index, columns):\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     columns = \u001b[43m_resolve_columns_of_interest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df.columns.is_unique:\n\u001b[32m    375\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    376\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mDuplicate column names found: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m.format(\u001b[38;5;28mlist\u001b[39m(df.columns))\n\u001b[32m    377\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/pyarrow/pandas_compat.py:546\u001b[39m, in \u001b[36m_resolve_columns_of_interest\u001b[39m\u001b[34m(df, schema, columns)\u001b[39m\n\u001b[32m    544\u001b[39m     columns = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m columns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns]\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     columns = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m columns\n",
      "\u001b[31mAttributeError\u001b[39m: 'Dataset' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Map evidence_labels to integers so that the Trainer will know what the labels mean\n",
    "label_dict = {\n",
    "    \"REFUTES\": 0,\n",
    "    \"NOT_ENOUGH_INFO\": 1,\n",
    "    \"SUPPORTS\": 2\n",
    "}\n",
    "\n",
    "preprocessed_df[\"labels\"] = preprocessed_df[\"evidence_label\"].map(label_dict)\n",
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f20ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(preprocessed_df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c52e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukeg/ClimatEnv/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"climatebert/distilroberta-base-climate-f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a206b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This splits our dataset so that we use 90% of it for training, and 10% for testing\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0dc017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e20a8389f09410da7778360e06f24bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7414611e278d464fb63c1c06d0b33761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than 100 per cent of the warming over the past century is due to human actions\n",
      "SUPPORTS\n",
      "SUPPORTS\n",
      "[0, 4321, 87, 727, 228, 715, 9, 5, 8232, 81, 5, 375, 3220, 16, 528, 7, 1050, 2163, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Human-produced carbon might be one of the factors [of climate change], but there’s simply no evidence that it is a significant one.\n",
      "REFUTES\n",
      "SUPPORTS\n",
      "[0, 33837, 12, 25617, 4363, 429, 28, 65, 9, 5, 1437, 50292, 646, 1116, 2147, 464, 7479, 53, 89, 50267, 29, 1622, 117, 1283, 14, 24, 16, 10, 1233, 65, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenize(examples):\n",
    "    # The code block below this one can be used to find what the max_length should be set to.\n",
    "    # Otherwise you have too much padding\n",
    "    # Consider this properly later\n",
    "    tokenized_output = tokenizer(\n",
    "        text=[f\"Claim: {claim} Evidence: {evidence}\" for claim, evidence in zip(examples[\"claim\"], examples[\"evidence\"])],\n",
    "        max_length=512, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True)\n",
    "\n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_training_dataset = split_dataset[\"train\"].map(custom_tokenize, batched=True)\n",
    "tokenized_testing_dataset = split_dataset[\"test\"].map(custom_tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Just for viewing purposes. Input_ids are the tokens, and attention_masks are whether they represent actual words or not.\n",
    "# The max_length is set to 512 so every entry has been padded to be this long, which may be unnecessary\n",
    "print(tokenized_training_dataset[6][\"claim\"])\n",
    "print(tokenized_training_dataset[6][\"evidence\"])\n",
    "print(tokenized_training_dataset[6][\"labels\"])\n",
    "print(tokenized_training_dataset[6][\"input_ids\"])\n",
    "print(tokenized_training_dataset[6][\"attention_mask\"])\n",
    "\n",
    "print(tokenized_testing_dataset[2][\"claim\"])\n",
    "print(tokenized_testing_dataset[2][\"evidence\"])\n",
    "print(tokenized_training_dataset[2][\"labels\"])\n",
    "print(tokenized_testing_dataset[2][\"input_ids\"])\n",
    "print(tokenized_testing_dataset[2][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25121d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to find the max token length in the dataset\n",
    "# Please reset max_length and padding parameters in above code cell before running this\n",
    "\n",
    "lengths = [len(tokens) for tokens in tokenized_training_dataset[\"input_ids\"]]\n",
    "max = 0\n",
    "for l in lengths:\n",
    "    if l > max:\n",
    "        max = l\n",
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bb039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Remove ignore_mismatched_sizes when needed - this replaces the head of the pretrained model (because if using\n",
    "# climateBERT/environmental-claims, it has already been fine tuned and has 2 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"climatebert/distilroberta-base-climate-f\",\n",
    "    num_labels=3,\n",
    "    # ignore_mismatched_sizes=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d8457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    cross_entropy_loss = CrossEntropyLoss(\n",
    "        torch.tensor(logits, dtype=torch.float32),\n",
    "        torch.tensor(labels, dtype=torch.long)\n",
    "    ).item()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"cross_entropy_loss\": cross_entropy_loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d66f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we set the hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/climateBERT-base/climate_fever_sample/check\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    # save_steps=50,\n",
    "    fp16=True,                          # Use 16-bit floating point instead of 32 - makes computation faster\n",
    "    warmup_ratio=0.1,                    # Allows the model to adapt a little\n",
    "\n",
    "    learning_rate=2e-5,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer  = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=tokenized_testing_dataset,\n",
    "    compute_metrics=calculate_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695eba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13cbaf0991e43e3b1266b04142d4f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:759\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type, prepend_batch_axis)\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m     tensor = \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    761\u001b[39m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[32m    762\u001b[39m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[32m    763\u001b[39m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[32m    764\u001b[39m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:721\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[39m\u001b[34m(value, dtype)\u001b[39m\n\u001b[32m    720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(np.array(value))\n\u001b[32m--> \u001b[39m\u001b[32m721\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/transformers/trainer.py:1859\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1857\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1864\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/transformers/trainer.py:2165\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2162\u001b[39m     rng_to_sync = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2164\u001b[39m step = -\u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2165\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   2168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/accelerate/data_loader.py:452\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/transformers/data/data_collator.py:271\u001b[39m, in \u001b[36mDataCollatorWithPadding.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    280\u001b[39m         batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m] = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/transformers/data/data_collator.py:66\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     padded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[32m     69\u001b[39m     tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = warning_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3355\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.pad\u001b[39m\u001b[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[39m\n\u001b[32m   3352\u001b[39m             batch_outputs[key] = []\n\u001b[32m   3353\u001b[39m         batch_outputs[key].append(value)\n\u001b[32m-> \u001b[39m\u001b[32m3355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:224\u001b[39m, in \u001b[36mBatchEncoding.__init__\u001b[39m\u001b[34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[39m\n\u001b[32m    220\u001b[39m     n_sequences = encoding[\u001b[32m0\u001b[39m].n_sequences\n\u001b[32m    222\u001b[39m \u001b[38;5;28mself\u001b[39m._n_sequences = n_sequences\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ClimatEnv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:775\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type, prepend_batch_axis)\u001b[39m\n\u001b[32m    770\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33moverflowing_tokens\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    771\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    772\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    773\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    774\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    776\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    777\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpadding=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtruncation=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    778\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    779\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m expected).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    780\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a1611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./results/climateBERT-base/climate_fever_sample/check\")\n",
    "# Please remember to delete model.safetensors BEFORE adding to git. Causes issues..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd43ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/climateBERT-base/climate_fever_sample/check/eval_metrics.json\", \"w\") as output_file:\n",
    "    json.dump(calculate_metrics, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ClimatEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
