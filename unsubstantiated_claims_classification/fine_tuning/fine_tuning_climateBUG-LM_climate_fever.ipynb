{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10860348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from preprocessing import climate_fever_to_claim_evidence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10e8a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# This should always output true now, but worth checking\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb9720bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The climate-fever dataset is in a format which the transformers Trainer does not understand\n",
    "# It must be preprocessed using the functions in preprocessing.py\n",
    "\n",
    "df = pd.read_json(\"/home/lukeg/Documents/VS_code/fine_tuning/lxg406/unsubstantiated_claims_classification/data/climate_fever/climate-fever-dataset-r1.jsonl\", lines=True)\n",
    "preprocessed_df = climate_fever_to_claim_evidence_pairs(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2264f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence_id</th>\n",
       "      <th>evidence_label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>entropy</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Global warming is driving polar bears toward e...</td>\n",
       "      <td>Extinction risk from global warming:170</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>\"Recent Research Shows Human Activity Driving ...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Global warming is driving polar bears toward e...</td>\n",
       "      <td>Global warming:14</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>Environmental impacts include the extinction o...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Global warming is driving polar bears toward e...</td>\n",
       "      <td>Global warming:178</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>Rising temperatures push bees to their physiol...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Global warming is driving polar bears toward e...</td>\n",
       "      <td>Habitat destruction:61</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>Rising global temperatures, caused by the gree...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Global warming is driving polar bears toward e...</td>\n",
       "      <td>Polar bear:1328</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>\"Bear hunting caught in global warming debate\".</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7670</th>\n",
       "      <td>3134</td>\n",
       "      <td>Over the last decade, heatwaves are five times...</td>\n",
       "      <td>Bushfires in Australia:126</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>Australia's climate has warmed by more than on...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7671</th>\n",
       "      <td>3134</td>\n",
       "      <td>Over the last decade, heatwaves are five times...</td>\n",
       "      <td>Effects of global warming:86</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>In the last 30–40 years, heat waves with high ...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7672</th>\n",
       "      <td>3134</td>\n",
       "      <td>Over the last decade, heatwaves are five times...</td>\n",
       "      <td>Global warming:155</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>Many regions have probably already seen increa...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7673</th>\n",
       "      <td>3134</td>\n",
       "      <td>Over the last decade, heatwaves are five times...</td>\n",
       "      <td>Global warming:156</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>Since the 1950s, droughts and heat waves have ...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7674</th>\n",
       "      <td>3134</td>\n",
       "      <td>Over the last decade, heatwaves are five times...</td>\n",
       "      <td>Heat wave:151</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>The effects of climate change have been projec...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7675 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      claim_id                                              claim  \\\n",
       "0            0  Global warming is driving polar bears toward e...   \n",
       "1            0  Global warming is driving polar bears toward e...   \n",
       "2            0  Global warming is driving polar bears toward e...   \n",
       "3            0  Global warming is driving polar bears toward e...   \n",
       "4            0  Global warming is driving polar bears toward e...   \n",
       "...        ...                                                ...   \n",
       "7670      3134  Over the last decade, heatwaves are five times...   \n",
       "7671      3134  Over the last decade, heatwaves are five times...   \n",
       "7672      3134  Over the last decade, heatwaves are five times...   \n",
       "7673      3134  Over the last decade, heatwaves are five times...   \n",
       "7674      3134  Over the last decade, heatwaves are five times...   \n",
       "\n",
       "                                  evidence_id   evidence_label  \\\n",
       "0     Extinction risk from global warming:170  NOT_ENOUGH_INFO   \n",
       "1                           Global warming:14         SUPPORTS   \n",
       "2                          Global warming:178  NOT_ENOUGH_INFO   \n",
       "3                      Habitat destruction:61         SUPPORTS   \n",
       "4                             Polar bear:1328  NOT_ENOUGH_INFO   \n",
       "...                                       ...              ...   \n",
       "7670               Bushfires in Australia:126         SUPPORTS   \n",
       "7671             Effects of global warming:86  NOT_ENOUGH_INFO   \n",
       "7672                       Global warming:155  NOT_ENOUGH_INFO   \n",
       "7673                       Global warming:156  NOT_ENOUGH_INFO   \n",
       "7674                            Heat wave:151         SUPPORTS   \n",
       "\n",
       "                                               evidence   entropy  labels  \n",
       "0     \"Recent Research Shows Human Activity Driving ...  0.693147       1  \n",
       "1     Environmental impacts include the extinction o...  0.000000       2  \n",
       "2     Rising temperatures push bees to their physiol...  0.693147       1  \n",
       "3     Rising global temperatures, caused by the gree...  0.000000       2  \n",
       "4       \"Bear hunting caught in global warming debate\".  0.693147       1  \n",
       "...                                                 ...       ...     ...  \n",
       "7670  Australia's climate has warmed by more than on...  0.000000       2  \n",
       "7671  In the last 30–40 years, heat waves with high ...  0.693147       1  \n",
       "7672  Many regions have probably already seen increa...  0.693147       1  \n",
       "7673  Since the 1950s, droughts and heat waves have ...  0.693147       1  \n",
       "7674  The effects of climate change have been projec...  0.000000       2  \n",
       "\n",
       "[7675 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map evidence_labels to integers so that the Trainer will know what the labels mean\n",
    "label_dict = {\n",
    "    \"REFUTES\": 0,\n",
    "    \"NOT_ENOUGH_INFO\": 1,\n",
    "    \"SUPPORTS\": 2\n",
    "}\n",
    "\n",
    "preprocessed_df[\"labels\"] = preprocessed_df[\"evidence_label\"].map(label_dict)\n",
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f20ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['claim_id', 'claim', 'evidence_id', 'evidence_label', 'evidence', 'entropy', 'labels'],\n",
       "    num_rows: 7675\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(preprocessed_df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41b73d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset! This randomly rearranges the dataset, which is good especially with this one since the same claim appears five times in a row\n",
    "# The seed parameter means we can access the exact same shuffle again if we need to\n",
    "seed=11\n",
    "dataset = dataset.shuffle(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2c52e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukeg/ClimatEnv/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"lumilogic/climateBUG-LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a206b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This splits our dataset so that we use 90% of it for training, and 10% for testing\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba0dc017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d5eff50e0b44e29a4667c1586f66ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6907 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc491899f0ca44b599f95035d49c7ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def custom_tokenize(examples):\n",
    "    # The code block below this one can be used to find what the max_length should be set to.\n",
    "    # Otherwise you have too much padding\n",
    "    # Consider this properly later\n",
    "    tokenized_output = tokenizer(\n",
    "        text=[f\"Claim: {claim} Evidence: {evidence}\" for claim, evidence in zip(examples[\"claim\"], examples[\"evidence\"])],\n",
    "        max_length=512, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True)\n",
    "\n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_training_dataset = split_dataset[\"train\"].map(custom_tokenize, batched=True)\n",
    "tokenized_testing_dataset = split_dataset[\"test\"].map(custom_tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c315ceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1905, PDO switched to a warm phase.\n",
      "During a \"warm\", or \"positive\", phase, the west Pacific becomes cooler and part of the eastern ocean warms; during a \"cool\" or \"negative\" phase, the opposite pattern occurs.\n",
      "1\n",
      "[0, 45699, 35, 96, 40849, 6, 11707, 673, 12012, 7, 10, 3279, 4359, 4, 27956, 35, 1590, 10, 22, 29530, 1297, 50, 22, 22173, 1297, 4359, 6, 5, 3072, 3073, 3374, 12924, 8, 233, 9, 5, 4580, 6444, 997, 4339, 131, 148, 10, 22, 24336, 113, 50, 22, 33407, 113, 4359, 6, 5, 5483, 6184, 11493, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "That humans are causing the rise in atmospheric CO2 is confirmed by multiple isotopic analyses.\n",
      "Higher atmospheric CO2 concentrations have led to an increase in dissolved CO2, which causes ocean acidification.\n",
      "1\n",
      "[0, 45699, 35, 280, 5868, 32, 3735, 5, 1430, 11, 24486, 6247, 176, 16, 1474, 30, 1533, 40640, 18137, 20070, 4, 27956, 35, 13620, 24486, 6247, 176, 26069, 33, 669, 7, 41, 712, 11, 25385, 6247, 176, 6, 61, 4685, 6444, 10395, 5000, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Just for viewing purposes. Input_ids are the tokens, and attention_masks are whether they represent actual words or not.\n",
    "# The max_length is set to 512 so every entry has been padded to be this long, which may be unnecessary\n",
    "print(tokenized_training_dataset[6][\"claim\"])\n",
    "print(tokenized_training_dataset[6][\"evidence\"])\n",
    "print(tokenized_training_dataset[6][\"labels\"])\n",
    "print(tokenized_training_dataset[6][\"input_ids\"])\n",
    "print(tokenized_training_dataset[6][\"attention_mask\"])\n",
    "\n",
    "print(tokenized_testing_dataset[2][\"claim\"])\n",
    "print(tokenized_testing_dataset[2][\"evidence\"])\n",
    "print(tokenized_training_dataset[2][\"labels\"])\n",
    "print(tokenized_testing_dataset[2][\"input_ids\"])\n",
    "print(tokenized_testing_dataset[2][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d94bb039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at lumilogic/climateBUG-LM and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Remove ignore_mismatched_sizes when needed - this replaces the head of the pretrained model (because if using\n",
    "# climateBERT/environmental-claims, it has already been fine tuned and has 2 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"lumilogic/climateBUG-LM\",\n",
    "    num_labels=3,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c97d8457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa99ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e75fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c75d66f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we set the hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/climateBUG-LM\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=600,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "    save_strategy=\"no\",\n",
    "    save_steps=500,\n",
    "    fp16=True,                          # Use 16-bit floating point instead of 32 - makes computation faster\n",
    "    warmup_ratio=0.05,                    # Allows the model to adapt a little\n",
    "    # gradient_accumulation_steps=2       # Might help with OOM errors, if we have them\n",
    "    learning_rate=3e-5,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "217e6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer  = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=tokenized_testing_dataset,\n",
    "    compute_metrics=calculate_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9695eba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae8a363929d468ebd5cc20d89d6e94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2592 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.886, 'grad_norm': 4.904855251312256, 'learning_rate': 2.9159220146222584e-05, 'epoch': 0.23}\n",
      "{'loss': 0.8571, 'grad_norm': 13.988655090332031, 'learning_rate': 2.6734362307067426e-05, 'epoch': 0.46}\n",
      "{'loss': 0.8334, 'grad_norm': 4.499449729919434, 'learning_rate': 2.429731925264013e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462502d1838e4904b52fa63d4ae369e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7635836005210876, 'eval_accuracy': 0.6875, 'eval_f1_score': 0.5969186999535802, 'eval_runtime': 7.0193, 'eval_samples_per_second': 109.413, 'eval_steps_per_second': 13.677, 'epoch': 0.69}\n",
      "{'loss': 0.7815, 'grad_norm': 12.039865493774414, 'learning_rate': 2.1860276198212834e-05, 'epoch': 0.93}\n",
      "{'loss': 0.7028, 'grad_norm': 9.814151763916016, 'learning_rate': 1.943541835905768e-05, 'epoch': 1.16}\n",
      "{'loss': 0.6705, 'grad_norm': 19.311227798461914, 'learning_rate': 1.6998375304630383e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e530bc696ed444a8d3a76570cd822be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7079771161079407, 'eval_accuracy': 0.6953125, 'eval_f1_score': 0.6904790374525714, 'eval_runtime': 7.0574, 'eval_samples_per_second': 108.822, 'eval_steps_per_second': 13.603, 'epoch': 1.39}\n",
      "{'loss': 0.6377, 'grad_norm': 8.336625099182129, 'learning_rate': 1.4561332250203087e-05, 'epoch': 1.62}\n",
      "{'loss': 0.6049, 'grad_norm': 19.59144401550293, 'learning_rate': 1.2124289195775792e-05, 'epoch': 1.85}\n",
      "{'loss': 0.5716, 'grad_norm': 21.828004837036133, 'learning_rate': 9.687246141348498e-06, 'epoch': 2.08}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d395624e154eb68eaffa1bb4fd85f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8048895001411438, 'eval_accuracy': 0.6783854166666666, 'eval_f1_score': 0.6823267483358283, 'eval_runtime': 6.8799, 'eval_samples_per_second': 111.629, 'eval_steps_per_second': 13.954, 'epoch': 2.08}\n",
      "{'loss': 0.4966, 'grad_norm': 17.55531883239746, 'learning_rate': 7.2502030869212026e-06, 'epoch': 2.31}\n",
      "{'loss': 0.4874, 'grad_norm': 31.529340744018555, 'learning_rate': 4.813160032493908e-06, 'epoch': 2.55}\n",
      "{'loss': 0.511, 'grad_norm': 8.42222785949707, 'learning_rate': 2.3761169780666128e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1816028be4d745d58a7cd2d494bbaa81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8075377345085144, 'eval_accuracy': 0.6809895833333334, 'eval_f1_score': 0.6825652975377808, 'eval_runtime': 7.1657, 'eval_samples_per_second': 107.177, 'eval_steps_per_second': 13.397, 'epoch': 2.78}\n",
      "{'train_runtime': 914.6052, 'train_samples_per_second': 22.656, 'train_steps_per_second': 2.834, 'train_loss': 0.6557252259902012, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2592, training_loss=0.6557252259902012, metrics={'train_runtime': 914.6052, 'train_samples_per_second': 22.656, 'train_steps_per_second': 2.834, 'total_flos': 2744905918178304.0, 'train_loss': 0.6557252259902012, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16a1611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"./results/climateBERT-base/climate_fever/first_run\")\n",
    "# Please remember to delete model.safetensors BEFORE adding to git. Causes issues...\n",
    "# Also it is probably not worth running this block until the model is worth keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cd43ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca8460503bb4ead912ae9fbc7b3e646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metrics are not included in the save model so we need to save them separately\n",
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f84e5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./results/climateBUG-LM/eval_metrics_seed{seed}.json\", \"w\") as output_file:\n",
    "    json.dump(metrics, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Climate Environment",
   "language": "python",
   "name": "climatenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
